{"title": "Carousel: A High-Resolution Dataset for Multi-Target Automatic Image Cropping", "authors": ["Rafe Loya", "Andrew Hamara", "Benjamin Estell", "Benjamin Kilpatrick", "Andrew C. Freeman"], "arxiv_id": "2511.04680v1", "summary": "Automatic image cropping is a method for maximizing the human-perceived\nquality of cropped regions in photographs. Although several works have proposed\ntechniques for producing singular crops, little work has addressed the problem\nof producing multiple, distinct crops with aesthetic appeal. In this paper, we\nmotivate the problem with a discussion on modern social media applications,\nintroduce a dataset of 277 relevant images and human labels, and evaluate the\nefficacy of several single-crop models with an image partitioning algorithm as\na pre-processing step. The dataset is available at\nhttps://github.com/RafeLoya/carousel.", "url": "http://arxiv.org/abs/2511.04680v1", "submitted_date": "2025-11-06 18:59:52", "categories": ["cs.CV"], "is_theoretical": "0", "keywords": ["自动图像裁剪", "多裁剪生成", "图像数据集", "美学裁剪", "社交媒体应用"]}
{"title": "GentleHumanoid: Learning Upper-body Compliance for Contact-rich Human and Object Interaction", "authors": ["Qingzhou Lu", "Yao Feng", "Baiyu Shi", "Michael Piseno", "Zhenan Bao", "C. Karen Liu"], "arxiv_id": "2511.04679v1", "summary": "Humanoid robots are expected to operate in human-centered environments where\nsafe and natural physical interaction is essential. However, most recent\nreinforcement learning (RL) policies emphasize rigid tracking and suppress\nexternal forces. Existing impedance-augmented approaches are typically\nrestricted to base or end-effector control and focus on resisting extreme\nforces rather than enabling compliance. We introduce GentleHumanoid, a\nframework that integrates impedance control into a whole-body motion tracking\npolicy to achieve upper-body compliance. At its core is a unified spring-based\nformulation that models both resistive contacts (restoring forces when pressing\nagainst surfaces) and guiding contacts (pushes or pulls sampled from human\nmotion data). This formulation ensures kinematically consistent forces across\nthe shoulder, elbow, and wrist, while exposing the policy to diverse\ninteraction scenarios. Safety is further supported through task-adjustable\nforce thresholds. We evaluate our approach in both simulation and on the\nUnitree G1 humanoid across tasks requiring different levels of compliance,\nincluding gentle hugging, sit-to-stand assistance, and safe object\nmanipulation. Compared to baselines, our policy consistently reduces peak\ncontact forces while maintaining task success, resulting in smoother and more\nnatural interactions. These results highlight a step toward humanoid robots\nthat can safely and effectively collaborate with humans and handle objects in\nreal-world environments.", "url": "http://arxiv.org/abs/2511.04679v1", "submitted_date": "2025-11-06 18:59:33", "categories": ["cs.RO", "cs.CV", "cs.HC"], "is_theoretical": "0", "keywords": ["人形机器人", "强化学习", "阻抗控制", "全身运动跟踪", "柔顺交互", "Unitree G1"]}
{"title": "Tracking and Understanding Object Transformations", "authors": ["Yihong Sun", "Xinyu Yang", "Jennifer J. Sun", "Bharath Hariharan"], "arxiv_id": "2511.04678v1", "summary": "Real-world objects frequently undergo state transformations. From an apple\nbeing cut into pieces to a butterfly emerging from its cocoon, tracking through\nthese changes is important for understanding real-world objects and dynamics.\nHowever, existing methods often lose track of the target object after\ntransformation, due to significant changes in object appearance. To address\nthis limitation, we introduce the task of Track Any State: tracking objects\nthrough transformations while detecting and describing state changes,\naccompanied by a new benchmark dataset, VOST-TAS. To tackle this problem, we\npresent TubeletGraph, a zero-shot system that recovers missing objects after\ntransformation and maps out how object states are evolving over time.\nTubeletGraph first identifies potentially overlooked tracks, and determines\nwhether they should be integrated based on semantic and proximity priors. Then,\nit reasons about the added tracks and generates a state graph describing each\nobserved transformation. TubeletGraph achieves state-of-the-art tracking\nperformance under transformations, while demonstrating deeper understanding of\nobject transformations and promising capabilities in temporal grounding and\nsemantic reasoning for complex object transformations. Code, additional\nresults, and the benchmark dataset are available at\nhttps://tubelet-graph.github.io.", "url": "http://arxiv.org/abs/2511.04678v1", "submitted_date": "2025-11-06 18:59:30", "categories": ["cs.CV"], "is_theoretical": "0", "keywords": ["物体状态跟踪", "Track Any State任务", "VOST-TAS数据集", "TubeletGraph系统", "零样本跟踪", "物体状态变换"]}
{"title": "InfinityStar: Unified Spacetime AutoRegressive Modeling for Visual Generation", "authors": ["Jinlai Liu", "Jian Han", "Bin Yan", "Hui Wu", "Fengda Zhu", "Xing Wang", "Yi Jiang", "Bingyue Peng", "Zehuan Yuan"], "arxiv_id": "2511.04675v1", "summary": "We introduce InfinityStar, a unified spacetime autoregressive framework for\nhigh-resolution image and dynamic video synthesis. Building on the recent\nsuccess of autoregressive modeling in both vision and language, our purely\ndiscrete approach jointly captures spatial and temporal dependencies within a\nsingle architecture. This unified design naturally supports a variety of\ngeneration tasks such as text-to-image, text-to-video, image-to-video, and long\ninteractive video synthesis via straightforward temporal autoregression.\nExtensive experiments demonstrate that InfinityStar scores 83.74 on VBench,\noutperforming all autoregressive models by large margins, even surpassing some\ndiffusion competitors like HunyuanVideo. Without extra optimizations, our model\ngenerates a 5s, 720p video approximately 10x faster than leading\ndiffusion-based methods. To our knowledge, InfinityStar is the first discrete\nautoregressive video generator capable of producing industrial level 720p\nvideos. We release all code and models to foster further research in efficient,\nhigh-quality video generation.", "url": "http://arxiv.org/abs/2511.04675v1", "submitted_date": "2025-11-06 18:58:03", "categories": ["cs.CV"], "is_theoretical": "0", "keywords": ["InfinityStar", "时空自回归框架", "高分辨率图像合成", "动态视频合成", "离散自回归模型", "视频生成任务"]}
{"title": "X-Diffusion: Training Diffusion Policies on Cross-Embodiment Human Demonstrations", "authors": ["Maximus A. Pace", "Prithwish Dan", "Chuanruo Ning", "Atiksh Bhardwaj", "Audrey Du", "Edward W. Duan", "Wei-Chiu Ma", "Kushal Kedia"], "arxiv_id": "2511.04671v1", "summary": "Human videos can be recorded quickly and at scale, making them an appealing\nsource of training data for robot learning. However, humans and robots differ\nfundamentally in embodiment, resulting in mismatched action execution. Direct\nkinematic retargeting of human hand motion can therefore produce actions that\nare physically infeasible for robots. Despite these low-level differences,\nhuman demonstrations provide valuable motion cues about how to manipulate and\ninteract with objects. Our key idea is to exploit the forward diffusion\nprocess: as noise is added to actions, low-level execution differences fade\nwhile high-level task guidance is preserved. We present X-Diffusion, a\nprincipled framework for training diffusion policies that maximally leverages\nhuman data without learning dynamically infeasible motions. X-Diffusion first\ntrains a classifier to predict whether a noisy action is executed by a human or\nrobot. Then, a human action is incorporated into policy training only after\nadding sufficient noise such that the classifier cannot discern its embodiment.\nActions consistent with robot execution supervise fine-grained denoising at low\nnoise levels, while mismatched human actions provide only coarse guidance at\nhigher noise levels. Our experiments show that naive co-training under\nexecution mismatches degrades policy performance, while X-Diffusion\nconsistently improves it. Across five manipulation tasks, X-Diffusion achieves\na 16% higher average success rate than the best baseline. The project website\nis available at https://portal-cornell.github.io/X-Diffusion/.", "url": "http://arxiv.org/abs/2511.04671v1", "submitted_date": "2025-11-06 18:56:30", "categories": ["cs.RO", "cs.AI", "cs.CV"], "is_theoretical": "0", "keywords": ["X-Diffusion", "人类视频数据", "机器人学习", "扩散策略", "动作重定向", "操作任务"]}
{"title": "Cambrian-S: Towards Spatial Supersensing in Video", "authors": ["Shusheng Yang", "Jihan Yang", "Pinzhi Huang", "Ellis Brown", "Zihao Yang", "Yue Yu", "Shengbang Tong", "Zihan Zheng", "Yifan Xu", "Muhan Wang", "Daohan Lu", "Rob Fergus", "Yann LeCun", "Li Fei-Fei", "Saining Xie"], "arxiv_id": "2511.04670v1", "summary": "We argue that progress in true multimodal intelligence calls for a shift from\nreactive, task-driven systems and brute-force long context towards a broader\nparadigm of supersensing. We frame spatial supersensing as four stages beyond\nlinguistic-only understanding: semantic perception (naming what is seen),\nstreaming event cognition (maintaining memory across continuous experiences),\nimplicit 3D spatial cognition (inferring the world behind pixels), and\npredictive world modeling (creating internal models that filter and organize\ninformation). Current benchmarks largely test only the early stages, offering\nnarrow coverage of spatial cognition and rarely challenging models in ways that\nrequire true world modeling. To drive progress in spatial supersensing, we\npresent VSI-SUPER, a two-part benchmark: VSR (long-horizon visual spatial\nrecall) and VSC (continual visual spatial counting). These tasks require\narbitrarily long video inputs yet are resistant to brute-force context\nexpansion. We then test data scaling limits by curating VSI-590K and training\nCambrian-S, achieving +30% absolute improvement on VSI-Bench without\nsacrificing general capabilities. Yet performance on VSI-SUPER remains limited,\nindicating that scale alone is insufficient for spatial supersensing. We\npropose predictive sensing as a path forward, presenting a proof-of-concept in\nwhich a self-supervised next-latent-frame predictor leverages surprise\n(prediction error) to drive memory and event segmentation. On VSI-SUPER, this\napproach substantially outperforms leading proprietary baselines, showing that\nspatial supersensing requires models that not only see but also anticipate,\nselect, and organize experience.", "url": "http://arxiv.org/abs/2511.04670v1", "submitted_date": "2025-11-06 18:55:17", "categories": ["cs.CV"], "is_theoretical": "0", "keywords": ["空间超感知", "VSI-SUPER", "VSR", "VSC", "预测感知", "多模态智能"]}
